{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2676b30-4cbc-4da4-8c21-e3d0403fb531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this code trains on all out of school students and provides code for production and results in risk profiles for all students\n",
    "# it outputs the numbers of students in each risk category for repeater risk and credit risk who are still in school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d78d536-a83e-468b-bd20-419f5b7af204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b08d668-0d98-427c-932c-bf684fb02796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the df - just be careful here as i had combined_data.csv in a different folder\n",
    "data = pd.read_csv(\"final_data/combined_data.csv\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76793308-6c2b-4cf9-afd7-582454bd6248",
   "metadata": {},
   "source": [
    "## Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45faa1a2-2c43-4285-8443-2db7ebbaf403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop some columns we just won't use\n",
    "\n",
    "def repeats_credits_data_processing(data):\n",
    "    data.drop(columns=\"MaskedStudentPersonKey\", inplace=True) # drops the student ID for the second time\n",
    "\n",
    "    data.drop(columns=['total_present', 'total_absent', \n",
    "                       'total_days', 'total_tardy', 'chronic_absent_10pct', # drop attendance data this is biasing the data\n",
    "                      #'graduated?', # drop graduated used for training/testing\n",
    "                      'test_score_2023', 'test_score_2024', 'test_score_2025', 'test_score_2026', # missing anyways\n",
    "                      'GradeLevel', # dropped bc leaky - indicates most recent level we have data for - which doesnt feel useful\n",
    "                      'present_pct' # just so much missing here\n",
    "                      ], inplace=True, errors='ignore')  \n",
    "\n",
    "\n",
    "\n",
    "    # combine the discipline data and drop the individual discpline cols\n",
    "    # combine discipline columns by taking the mean\n",
    "    discipline_cols = ['num_discipline_2023', 'num_discipline_2024',\n",
    "                       'num_discipline_2025', 'num_discipline_2026']\n",
    "\n",
    "    # set all discipline not explicitly given 0 instead of NaN - the data supports this as only students with discipline events have any numbers - students\n",
    "    # without discipline incidents have NaNS all the way across\n",
    "    data[discipline_cols] = data[discipline_cols].fillna(0)\n",
    "\n",
    "    data['mean_discipline'] = data[discipline_cols].mean(axis=1, skipna=True)\n",
    "\n",
    "    # drop the individual discipline columns\n",
    "    data = data.drop(columns=discipline_cols)\n",
    "\n",
    "    # get rid of MS columns\n",
    "    # drop all columns that start with 'MS_'\n",
    "    cols_to_drop = [col for col in data.columns if col.startswith('MS_')]\n",
    "    data_clean = data.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # students no longer in school\n",
    "    out_of_school_df = data_clean[(data_clean['DropOut'] == 1) | (data_clean['graduated?'] == 1)].reset_index(drop=True)\n",
    "\n",
    "    # students still in school \n",
    "    in_school_df = data_clean[(data_clean['DropOut'] != 1) & (data_clean['graduated?'] != 1)]\n",
    "    \n",
    "    return out_of_school_df, in_school_df\n",
    "\n",
    "repeats_credits_outschool_df, repeats_credits_inschool_df = repeats_credits_data_processing(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80688791-97eb-4c0a-ba7f-42c4536e1a71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dataframe for binned credits\n",
    "\n",
    "def make_filtered_credits_repeats_df(df):\n",
    "    # create a new dataframe for binning credits earned - for training/testing later\n",
    "    binned_credits_earned_df = df.copy()\n",
    "    binned_credits_earned_df['Not all credits'] = (binned_credits_earned_df['HS_PctEarned'] < 0.8).astype(int) #0.999999\n",
    "    # display(binned_credits_earned_df)\n",
    "    \n",
    "    binned_credits_clean = binned_credits_earned_df.dropna(subset=['mean_discipline', 'MAP_TestRITScore', 'HS_SenseofBelonging',\n",
    "                                                              ]).reset_index(drop=True)\n",
    "    \n",
    "    repeat_clean = df.dropna(subset=['HS_PctEarned', 'mean_discipline', 'MAP_TestRITScore', 'HS_SenseofBelonging']).reset_index(drop=True)\n",
    "    \n",
    "    return binned_credits_clean, repeat_clean\n",
    "\n",
    "binned_credits_clean, repeat_clean = make_filtered_credits_repeats_df(repeats_credits_outschool_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a752f133-d929-4a3c-ae5e-44433b2d97be",
   "metadata": {},
   "source": [
    "## Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eb2e9ce-2be8-4cae-bbce-39ecd6df3f29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def credits_repeats_train_logistic_regression_full(df, target_col, feature_cols, random_state=42):\n",
    "    # select features and target\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # scale\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)  # on all training data\n",
    "\n",
    "    # train\n",
    "    clf = LogisticRegression(max_iter=1000, random_state=random_state, class_weight='balanced')\n",
    "    clf.fit(X_scaled, y)\n",
    "\n",
    "    # print(\"Model trained on all available data.\")\n",
    "    return clf, scaler\n",
    "\n",
    "def credits_repeats_apply_logistic_model(clf, scaler, new_df, feature_cols):\n",
    "    # apply the same scaler (don't fit again!)\n",
    "    X_new_scaled = scaler.transform(new_df[feature_cols])\n",
    "    \n",
    "    # predict probabilities\n",
    "    new_df['pred_proba'] = clf.predict_proba(X_new_scaled)[:, 1]\n",
    "    new_df['pred_label'] = clf.predict(X_new_scaled)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def credits_repeats_assign_risk_tiers_by_percentile(df, prob_col='pred_proba'):\n",
    "    # sort by probability\n",
    "    df = df.sort_values(prob_col, ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    # define percentile cut points (quintiles)\n",
    "    quantiles = df[prob_col].quantile([0.2, 0.4, 0.6, 0.8]).values\n",
    "    \n",
    "    # assign tiers\n",
    "    def get_tier(prob):\n",
    "        if prob <= 0.2:\n",
    "            return \"Low Risk\"\n",
    "        elif prob <= 0.4:\n",
    "            return \"Moderately Low Risk\"\n",
    "        elif prob <= 0.6:\n",
    "            return \"Moderate Risk\"\n",
    "        elif prob <= 0.8:\n",
    "            return \"Moderately High Risk\"\n",
    "        else:\n",
    "            return \"High Risk\"\n",
    "    \n",
    "    df['risk_tier'] = df[prob_col].apply(get_tier)\n",
    "    \n",
    "    # summary: number of students in each tier, sorted Low to High\n",
    "    tier_order = [\"Low Risk\", \"Moderately Low Risk\", \"Moderate Risk\", \n",
    "                  \"Moderately High Risk\", \"High Risk\"]\n",
    "    summary = df['risk_tier'].value_counts().reindex(tier_order).fillna(0).astype(int)\n",
    "    \n",
    "    return df, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c050ae5-ea35-4dd7-9fa6-80e1f3bac3c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "risk_tier\n",
      "Low Risk                12107\n",
      "Moderately Low Risk      3571\n",
      "Moderate Risk            1602\n",
      "Moderately High Risk     1221\n",
      "High Risk                2180\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# REPEATER PRODUCTION\n",
    "# 1. train on full data\n",
    "repeat_clf, repeat_scaler = credits_repeats_train_logistic_regression_full(\n",
    "    df=repeat_clean,\n",
    "    target_col='HS_IsRepeater',\n",
    "    feature_cols=['HS_PctEarned', 'mean_discipline', 'MAP_TestRITScore', 'HS_SenseofBelonging']\n",
    ")\n",
    "\n",
    "# 2. apply to unseen data\n",
    "repeat_predicted_new_data = credits_repeats_apply_logistic_model(\n",
    "    repeat_clf,\n",
    "    repeat_scaler,\n",
    "    new_df=repeats_credits_inschool_df.dropna(subset=['HS_PctEarned', 'mean_discipline', 'MAP_TestRITScore', 'HS_SenseofBelonging']).reset_index(drop=True),\n",
    "    feature_cols=['HS_PctEarned', 'mean_discipline', 'MAP_TestRITScore', 'HS_SenseofBelonging']\n",
    ")\n",
    "\n",
    "# apply to data\n",
    "repeat_df_with_tiers, repeat_risk_summary = credits_repeats_assign_risk_tiers_by_percentile(repeat_predicted_new_data, prob_col='pred_proba')\n",
    "print(repeat_risk_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96f1367e-bef0-42f3-9110-e022a9bd3304",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "risk_tier\n",
      "Low Risk                3316\n",
      "Moderately Low Risk     6759\n",
      "Moderate Risk           5101\n",
      "Moderately High Risk    3755\n",
      "High Risk               2434\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# CREDITS PRODUCTION\n",
    "# 1. train on full data\n",
    "credit_clf, credit_scaler = credits_repeats_train_logistic_regression_full(\n",
    "    df=binned_credits_clean,    \n",
    "    target_col='Not all credits',\n",
    "    feature_cols=['mean_discipline', 'MAP_TestRITScore', 'HS_SenseofBelonging']\n",
    ")\n",
    "\n",
    "# 2. apply to unseen data\n",
    "credits_predicted_new_data = credits_repeats_apply_logistic_model(\n",
    "    credit_clf,\n",
    "    credit_scaler,\n",
    "    new_df=repeats_credits_inschool_df.dropna(subset=['mean_discipline', 'MAP_TestRITScore', 'HS_SenseofBelonging',\n",
    "                ]).reset_index(drop=True),\n",
    "    feature_cols=['mean_discipline', 'MAP_TestRITScore', 'HS_SenseofBelonging']\n",
    ")\n",
    "\n",
    "# apply to data\n",
    "credit_df_with_tiers, credit_risk_summary = credits_repeats_assign_risk_tiers_by_percentile(credits_predicted_new_data, prob_col='pred_proba')\n",
    "print(credit_risk_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
